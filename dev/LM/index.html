<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Statistical Language Model · TextAnalysis</title><link rel="canonical" href="https://juliatext.github.io/TextAnalysis.jl/stable/LM/index.html"/><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/documenter.css" rel="stylesheet" type="text/css"/><link href="../assets/custom.css" rel="stylesheet" type="text/css"/><script src="../assets/custom.js"></script><link href="../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><nav class="toc"><h1>TextAnalysis</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../">Home</a></li><li><a class="toctext" href="../documents/">Documents</a></li><li><a class="toctext" href="../corpus/">Corpus</a></li><li><a class="toctext" href="../features/">Features</a></li><li><a class="toctext" href="../semantic/">Semantic Analysis</a></li><li><a class="toctext" href="../classify/">Classifier</a></li><li><a class="toctext" href="../example/">Extended Example</a></li><li><a class="toctext" href="../evaluation_metrics/">Evaluation Metrics</a></li><li><a class="toctext" href="../crf/">Conditional Random Fields</a></li><li><a class="toctext" href="../ner/">Named Entity Recognition</a></li><li><a class="toctext" href="../ULMFiT/">ULMFiT</a></li><li class="current"><a class="toctext" href>Statistical Language Model</a><ul class="internal"><li><a class="toctext" href="#APIs-1">APIs</a></li><li><a class="toctext" href="#Evaluation-Method-1">Evaluation Method</a></li><li class="toplevel"><a class="toctext" href="#lookup-a-sequence-or-words-in-the-vocabulary-1">lookup a sequence or words in the vocabulary</a></li></ul></li><li><a class="toctext" href="../APIReference/">API References</a></li></ul></nav><article id="docs"><header><nav><ul><li><a href>Statistical Language Model</a></li></ul><a class="edit-page" href="https://github.com/JuliaText/TextAnalysis.jl/blob/master/docs/src/LM.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Statistical Language Model</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="Statistical-Language-Model-1" href="#Statistical-Language-Model-1">Statistical Language Model</a></h1><p><strong>TextAnalysis</strong> provide following different Language Models </p><ul><li><strong>MLE</strong> - Base Ngram model.</li><li><strong>Lidstone</strong> - Base Ngram model with Lidstone smoothing.</li><li><strong>Laplace</strong> - Base Ngram language model with Laplace smoothing.</li><li><strong>WittenBellInterpolated</strong> - Interpolated Version of witten-Bell algorithm.</li><li><strong>KneserNeyInterpolated</strong> - Interpolated  version of Kneser -Ney smoothing.</li></ul><h2><a class="nav-anchor" id="APIs-1" href="#APIs-1">APIs</a></h2><p>To use the API, we first <em>Instantiate</em> desired model and then load it with train set</p><pre><code class="language-julia">MLE(word::Vector{T}, unk_cutoff=1, unk_label=&quot;&lt;unk&gt;&quot;) where { T &lt;: AbstractString}
        
Lidstone(word::Vector{T}, gamma:: Float64, unk_cutoff=1, unk_label=&quot;&lt;unk&gt;&quot;) where { T &lt;: AbstractString}
        
Laplace(word::Vector{T}, unk_cutoff=1, unk_label=&quot;&lt;unk&gt;&quot;) where { T &lt;: AbstractString}
        
WittenBellInterpolated(word::Vector{T}, unk_cutoff=1, unk_label=&quot;&lt;unk&gt;&quot;) where { T &lt;: AbstractString}
        
KneserNeyInterpolated(word::Vector{T}, discount:: Float64=0.1, unk_cutoff=1, unk_label=&quot;&lt;unk&gt;&quot;) where { T &lt;: AbstractString}
        
(lm::&lt;Languagemodel&gt;)(text, min::Integer, max::Integer)</code></pre><p>Arguments:</p><ul><li><p><code>word</code> : Array of  strings to store vocabulary.</p></li><li><p><code>unk_cutoff</code>: Tokens with counts greater than or equal to the cutoff value will be considered part of the vocabulary.</p></li><li><p><code>unk_label</code>: token for unkown labels </p></li><li><p><code>gamma</code>: smoothing arugment gamma </p></li><li><p><code>discount</code>:  discounting factor for <code>KneserNeyInterpolated</code></p><p>for more information see docstrings of vocabulary</p></li></ul><pre><code class="language-julia">julia&gt; voc = [&quot;my&quot;,&quot;name&quot;,&quot;is&quot;,&quot;salman&quot;,&quot;khan&quot;,&quot;and&quot;,&quot;he&quot;,&quot;is&quot;,&quot;shahrukh&quot;,&quot;Khan&quot;]

julia&gt; train = [&quot;khan&quot;,&quot;is&quot;,&quot;my&quot;,&quot;good&quot;, &quot;friend&quot;,&quot;and&quot;,&quot;He&quot;,&quot;is&quot;,&quot;my&quot;,&quot;brother&quot;]
# voc and train are used to train vocabulary and model respectively

julia&gt; model = MLE(voc)
MLE(Vocabulary(Dict(&quot;khan&quot;=&gt;1,&quot;name&quot;=&gt;1,&quot;&lt;unk&gt;&quot;=&gt;1,&quot;salman&quot;=&gt;1,&quot;is&quot;=&gt;2,&quot;Khan&quot;=&gt;1,&quot;my&quot;=&gt;1,&quot;he&quot;=&gt;1,&quot;shahrukh&quot;=&gt;1,&quot;and&quot;=&gt;1…), 1, &quot;&lt;unk
        &gt;&quot;, [&quot;my&quot;, &quot;name&quot;, &quot;is&quot;, &quot;salman&quot;, &quot;khan&quot;, &quot;and&quot;, &quot;he&quot;, &quot;is&quot;, &quot;shahrukh&quot;, &quot;Khan&quot;, &quot;&lt;unk&gt;&quot;]))
julia&gt; print(voc)
11-element Array{String,1}:
 &quot;my&quot;      
 &quot;name&quot;    
 &quot;is&quot;      
 &quot;salman&quot;  
 &quot;khan&quot;    
 &quot;and&quot;     
 &quot;he&quot;      
 &quot;is&quot;      
 &quot;shahrukh&quot;
 &quot;Khan&quot;    
 &quot;&lt;unk&gt;&quot;   
# you can see &quot;&lt;unk&gt;&quot; token is added to voc 
julia&gt; fit = model(train,2,2) #considering only bigrams
julia&gt; unmaskedscore = score(model, fit, &quot;is&quot; ,&quot;&lt;unk&gt;&quot;) #score output P(word | context) without replacing context word with &quot;&lt;unk&gt;&quot;
0.3333333333333333
julia&gt; masked_score = maskedscore(model,fit,&quot;is&quot;,&quot;alien&quot;)
0.3333333333333333
#as expected maskedscore is equivalent to unmaskedscore with context replaced with &quot;&lt;unk&gt;&quot;
</code></pre><div class="admonition note"><div class="admonition-title">Note</div><div class="admonition-text"><p>When you call <code>MLE(voc)</code> for the first time, It will update your vocabulary set as well. </p></div></div><h2><a class="nav-anchor" id="Evaluation-Method-1" href="#Evaluation-Method-1">Evaluation Method</a></h2><h3><a class="nav-anchor" id="score-1" href="#score-1"><code>score</code></a></h3><pre><code class="language-none">used to evaluate the probability of word given context (*P(word | context)*)</code></pre><pre><code class="language-julia">	score(m::gammamodel, temp_lm::DefaultDict, word::AbstractString, context::AbstractString)</code></pre><p>Arguments:                                                        </p><ol><li><code>m</code> : Instance of <code>Langmodel</code> struct.</li><li><code>temp_lm</code>: output of function call of instance of <code>Langmodel</code>.</li><li><code>word</code>: string of word </li><li><code>context</code>: context of given word</li></ol><p>​	In case of Lidstone and Laplace it apply smoothing and, </p><p>​	In Interpolated language model, provide Kneserney and WittenBell smoothing  </p><h3><a class="nav-anchor" id="maskedscore-1" href="#maskedscore-1"><code>maskedscore</code></a></h3><p>It is used to evaluate <em>score</em> with masks out of vocabulary words</p><p>The arguments are the same as for score</p><h3><a class="nav-anchor" id="logscore-1" href="#logscore-1"><code>logscore</code></a></h3><p>Evaluate the log score of this word in this context.</p><p>The arguments are the same as for score and maskedscore</p><h3><a class="nav-anchor" id="entropy-1" href="#entropy-1"><code>entropy</code></a></h3><pre><code class="language-julia">  entropy(m::Langmodel,lm::DefaultDict,text_ngram::word::Vector{T}) where { T &lt;: AbstractString}
	```

  Calculate cross-entropy of model for given evaluation text.

  Input text must be Array of ngram of same lengths

### `perplexity`  

  Calculates the perplexity of the given text.

  This is simply 2 ** cross-entropy(`entropy`) for the text, so the arguments are the same as `entropy`.

##  Preprocessing

 For Preprocessing following functions:

1. `everygram`: Return all possible ngrams generated from sequence of items, as an Array{String,1}

 ```julia
   julia&gt; seq = [&quot;To&quot;,&quot;be&quot;,&quot;or&quot;,&quot;not&quot;]
   julia&gt; a = everygram(seq,min_len=1, max_len=-1)
    10-element Array{Any,1}:
     &quot;or&quot;          
     &quot;not&quot;         
     &quot;To&quot;          
     &quot;be&quot;                  
     &quot;or not&quot; 
     &quot;be or&quot;       
     &quot;be or not&quot;   
     &quot;To be or&quot;    
     &quot;To be or not&quot;
 ```

2. `padding_ngrams`: padding _ngram is used to pad both left and right of sentence and out putting ngrmas of order n

   It also pad the original input Array of string 

 ```julia
   julia&gt; example = [&quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,&quot;5&quot;]
   julia&gt; padding_ngrams(example,2,pad_left=true,pad_right=true)
    6-element Array{Any,1}:
     &quot;&lt;s&gt; 1&quot; 
     &quot;1 2&quot;   
     &quot;2 3&quot;   
     &quot;3 4&quot;   
     &quot;4 5&quot;   
     &quot;5 &lt;/s&gt;&quot;
 ```
## Vocabulary 

Struct to store Language models vocabulary

checking membership and filters items by comparing their counts to a cutoff value

It also Adds a special &quot;unkown&quot; tokens which unseen words are mapped to
</code></pre><p>julia julia&gt; words = [&quot;a&quot;, &quot;c&quot;, &quot;-&quot;, &quot;d&quot;, &quot;c&quot;, &quot;a&quot;, &quot;b&quot;, &quot;r&quot;, &quot;a&quot;, &quot;c&quot;, &quot;d&quot;] julia&gt; vocabulary = Vocabulary(words, 2)    Vocabulary(Dict(&quot;&lt;unk&gt;&quot;=&gt;1,&quot;c&quot;=&gt;3,&quot;a&quot;=&gt;3,&quot;d&quot;=&gt;2), 2, &quot;&lt;unk&gt;&quot;) </p><h1><a class="nav-anchor" id="lookup-a-sequence-or-words-in-the-vocabulary-1" href="#lookup-a-sequence-or-words-in-the-vocabulary-1">lookup a sequence or words in the vocabulary</a></h1><p>julia&gt; word = [&quot;a&quot;, &quot;-&quot;, &quot;d&quot;, &quot;c&quot;, &quot;a&quot;]</p><p>julia&gt; lookup(vocabulary ,word)  5-element Array{Any,1}:   &quot;a&quot;       &quot;&lt;unk&gt;&quot;   &quot;d&quot;       &quot;c&quot;       &quot;a&quot; ```</p><footer><hr/><a class="previous" href="../ULMFiT/"><span class="direction">Previous</span><span class="title">ULMFiT</span></a><a class="next" href="../APIReference/"><span class="direction">Next</span><span class="title">API References</span></a></footer></article></body></html>
