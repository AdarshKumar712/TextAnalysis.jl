<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Features · TextAnalysis</title><link rel="canonical" href="https://juliatext.github.io/TextAnalysis.jl/stable/features/index.html"/><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/documenter.css" rel="stylesheet" type="text/css"/><link href="../assets/custom.css" rel="stylesheet" type="text/css"/><script src="../assets/custom.js"></script></head><body><nav class="toc"><h1>TextAnalysis</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../">Home</a></li><li><a class="toctext" href="../documents/">Documents</a></li><li><a class="toctext" href="../corpus/">Corpus</a></li><li class="current"><a class="toctext" href>Features</a><ul class="internal"><li><a class="toctext" href="#Creating-a-Document-Term-Matrix-1">Creating a Document Term Matrix</a></li><li><a class="toctext" href="#Creating-Individual-Rows-of-a-Document-Term-Matrix-1">Creating Individual Rows of a Document Term Matrix</a></li><li><a class="toctext" href="#The-Hash-Trick-1">The Hash Trick</a></li><li><a class="toctext" href="#TF-(Term-Frequency)-1">TF (Term Frequency)</a></li><li><a class="toctext" href="#TF-IDF-(Term-Frequency-Inverse-Document-Frequency)-1">TF-IDF (Term Frequency - Inverse Document Frequency)</a></li><li><a class="toctext" href="#Okapi-BM-25-1">Okapi BM-25</a></li><li><a class="toctext" href="#Co-occurrence-matrix-(COOM)-1">Co occurrence matrix (COOM)</a></li><li><a class="toctext" href="#Sentiment-Analyzer-1">Sentiment Analyzer</a></li><li><a class="toctext" href="#Summarizer-1">Summarizer</a></li><li><a class="toctext" href="#Tagging_schemes-1">Tagging_schemes</a></li><li><a class="toctext" href="#Parts-of-Speech-Tagger-1">Parts of Speech Tagger</a></li></ul></li><li><a class="toctext" href="../semantic/">Semantic Analysis</a></li><li><a class="toctext" href="../classify/">Classifier</a></li><li><a class="toctext" href="../example/">Extended Example</a></li><li><a class="toctext" href="../evaluation_metrics/">Evaluation Metrics</a></li><li><a class="toctext" href="../crf/">Conditional Random Fields</a></li><li><a class="toctext" href="../ner/">Named Entity Recognition</a></li></ul></nav><article id="docs"><header><nav><ul><li><a href>Features</a></li></ul><a class="edit-page" href="https://github.com/JuliaText/TextAnalysis.jl/blob/master/docs/src/features.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Features</span><a class="fa fa-bars" href="#"></a></div></header><h2><a class="nav-anchor" id="Creating-a-Document-Term-Matrix-1" href="#Creating-a-Document-Term-Matrix-1">Creating a Document Term Matrix</a></h2><p>Often we want to represent documents as a matrix of word counts so that we can apply linear algebra operations and statistical techniques. Before we do this, we need to update the lexicon:</p><pre><code class="language-julia">julia&gt; crps = Corpus([StringDocument(&quot;To be or not to be&quot;),
                             StringDocument(&quot;To become or not to become&quot;)])

julia&gt; update_lexicon!(crps)

julia&gt; m = DocumentTermMatrix(crps)
A 2 X 6 DocumentTermMatrix</code></pre><p>A <code>DocumentTermMatrix</code> object is a special type. If you would like to use a simple sparse matrix, call <code>dtm()</code> on this object:</p><pre><code class="language-julia">julia&gt; dtm(m)
2×6 SparseArrays.SparseMatrixCSC{Int64,Int64} with 10 stored entries:
  [1, 1]  =  1
  [2, 1]  =  1
  [1, 2]  =  2
  [2, 3]  =  2
  [1, 4]  =  1
  [2, 4]  =  1
  [1, 5]  =  1
  [2, 5]  =  1
  [1, 6]  =  1
  [2, 6]  =  1</code></pre><p>If you would like to use a dense matrix instead, you can pass this as an argument to the <code>dtm</code> function:</p><pre><code class="language-julia">julia&gt; dtm(m, :dense)
2×6 Array{Int64,2}:
 1  2  0  1  1  1
 1  0  2  1  1  1</code></pre><h2><a class="nav-anchor" id="Creating-Individual-Rows-of-a-Document-Term-Matrix-1" href="#Creating-Individual-Rows-of-a-Document-Term-Matrix-1">Creating Individual Rows of a Document Term Matrix</a></h2><p>In many cases, we don&#39;t need the entire document term matrix at once: we can make do with just a single row. You can get this using the <code>dtv</code> function. Because individual&#39;s document do not have a lexicon associated with them, we have to pass in a lexicon as an additional argument:</p><pre><code class="language-julia">julia&gt; dtv(crps[1], lexicon(crps))
1×6 Array{Int64,2}:
 1  2  0  1  1  1</code></pre><h2><a class="nav-anchor" id="The-Hash-Trick-1" href="#The-Hash-Trick-1">The Hash Trick</a></h2><p>The need to create a lexicon before we can construct a document term matrix is often prohibitive. We can often employ a trick that has come to be called the &quot;Hash Trick&quot; in which we replace terms with their hashed valued using a hash function that outputs integers from 1 to N. To construct such a hash function, you can use the <code>TextHashFunction(N)</code> constructor:</p><pre><code class="language-julia">julia&gt; h = TextHashFunction(10)
TextHashFunction(hash, 10)</code></pre><p>You can see how this function maps strings to numbers by calling the <code>index_hash</code> function:</p><pre><code class="language-julia">julia&gt; index_hash(&quot;a&quot;, h)
8

julia&gt; index_hash(&quot;b&quot;, h)
7</code></pre><p>Using a text hash function, we can represent a document as a vector with N entries by calling the <code>hash_dtv</code> function:</p><pre><code class="language-julia">julia&gt; hash_dtv(crps[1], h)
1×10 Array{Int64,2}:
 0  2  0  0  1  3  0  0  0  0</code></pre><p>This can be done for a corpus as a whole to construct a DTM without defining a lexicon in advance:</p><pre><code class="language-julia">julia&gt; hash_dtm(crps, h)
2×10 Array{Int64,2}:
 0  2  0  0  1  3  0  0  0  0
 0  2  0  0  1  1  0  0  2  0</code></pre><p>Every corpus has a hash function built-in, so this function can be called using just one argument:</p><pre><code class="language-julia">julia&gt; hash_dtm(crps)
2×100 Array{Int64,2}:
 0  0  0  0  0  0  0  0  0  0  0  0  0  …  0  0  0  0  0  0  0  0  0  0  0  0
 0  0  0  0  0  0  0  0  2  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0</code></pre><p>Moreover, if you do not specify a hash function for just one row of the hash DTM, a default hash function will be constructed for you:</p><pre><code class="language-julia">julia&gt; hash_dtv(crps[1])
1×100 Array{Int64,2}:
 0  0  0  0  0  0  0  0  0  0  0  0  0  …  0  0  0  0  0  0  0  0  0  0  0  0</code></pre><h2><a class="nav-anchor" id="TF-(Term-Frequency)-1" href="#TF-(Term-Frequency)-1">TF (Term Frequency)</a></h2><p>Often we need to find out the proportion of a document is contributed by each term. This can be done by finding the term frequency function</p><pre><code class="language-none">tf(dtm)</code></pre><p>The parameter, <code>dtm</code> can be of the types - <code>DocumentTermMatrix</code> , <code>SparseMatrixCSC</code> or <code>Matrix</code></p><pre><code class="language-julia">julia&gt; crps = Corpus([StringDocument(&quot;To be or not to be&quot;),
              StringDocument(&quot;To become or not to become&quot;)])

julia&gt; update_lexicon!(crps)

julia&gt; m = DocumentTermMatrix(crps)

julia&gt; tf(m)
2×6 SparseArrays.SparseMatrixCSC{Float64,Int64} with 10 stored entries:
  [1, 1]  =  0.166667
  [2, 1]  =  0.166667
  [1, 2]  =  0.333333
  [2, 3]  =  0.333333
  [1, 4]  =  0.166667
  [2, 4]  =  0.166667
  [1, 5]  =  0.166667
  [2, 5]  =  0.166667
  [1, 6]  =  0.166667
  [2, 6]  =  0.166667</code></pre><h2><a class="nav-anchor" id="TF-IDF-(Term-Frequency-Inverse-Document-Frequency)-1" href="#TF-IDF-(Term-Frequency-Inverse-Document-Frequency)-1">TF-IDF (Term Frequency - Inverse Document Frequency)</a></h2><pre><code class="language-none">tf_idf(dtm)</code></pre><p>In many cases, raw word counts are not appropriate for use because:</p><ul><li>(A) Some documents are longer than other documents</li><li>(B) Some words are more frequent than other words</li></ul><p>You can work around this by performing TF-IDF on a DocumentTermMatrix:</p><pre><code class="language-julia">julia&gt; crps = Corpus([StringDocument(&quot;To be or not to be&quot;),
              StringDocument(&quot;To become or not to become&quot;)])

julia&gt; update_lexicon!(crps)

julia&gt; m = DocumentTermMatrix(crps)
DocumentTermMatrix(
  [1, 1]  =  1
  [2, 1]  =  1
  [1, 2]  =  2
  [2, 3]  =  2
  [1, 4]  =  1
  [2, 4]  =  1
  [1, 5]  =  1
  [2, 5]  =  1
  [1, 6]  =  1
  [2, 6]  =  1, [&quot;To&quot;, &quot;be&quot;, &quot;become&quot;, &quot;not&quot;, &quot;or&quot;, &quot;to&quot;], Dict(&quot;or&quot;=&gt;5,&quot;not&quot;=&gt;4,&quot;to&quot;=&gt;6,&quot;To&quot;=&gt;1,&quot;be&quot;=&gt;2,&quot;become&quot;=&gt;3))

julia&gt; tf_idf(m)
2×6 SparseArrays.SparseMatrixCSC{Float64,Int64} with 10 stored entries:
  [1, 1]  =  0.0
  [2, 1]  =  0.0
  [1, 2]  =  0.231049
  [2, 3]  =  0.231049
  [1, 4]  =  0.0
  [2, 4]  =  0.0
  [1, 5]  =  0.0
  [2, 5]  =  0.0
  [1, 6]  =  0.0
  [2, 6]  =  0.0</code></pre><p>As you can see, TF-IDF has the effect of inserting 0&#39;s into the columns of words that occur in all documents. This is a useful way to avoid having to remove those words during preprocessing.</p><h2><a class="nav-anchor" id="Okapi-BM-25-1" href="#Okapi-BM-25-1">Okapi BM-25</a></h2><p>From the document term matparamterix, <a href="https://en.wikipedia.org/wiki/Okapi_BM25">Okapi BM25</a> document-word statistic can be created.</p><pre><code class="language-none">bm_25(dtm::AbstractMatrix; κ, β)
bm_25(dtm::DocumentTermMatrixm, κ, β)</code></pre><p>It can also be used via the following methods Overwrite the <code>bm25</code> with calculated weights.</p><pre><code class="language-none">bm_25!(dtm, bm25, κ, β)</code></pre><p>The inputs matrices can also be a <code>Sparse Matrix</code>. The parameters κ and β default to 2 and 0.75 respectively.</p><p>Here is an example usage -</p><pre><code class="language-julia">julia&gt; crps = Corpus([StringDocument(&quot;a a a sample text text&quot;), StringDocument(&quot;another example example text text&quot;), StringDocument(&quot;&quot;), StringDocument(&quot;another another text text text text&quot;)])

julia&gt; update_lexicon!(crps)

julia&gt; m = DocumentTermMatrix(crps)

julia&gt; bm_25(m)
4×5 SparseArrays.SparseMatrixCSC{Float64,Int64} with 8 stored entries:
  [1, 1]  =  1.29959
  [2, 2]  =  0.882404
  [4, 2]  =  1.40179
  [2, 3]  =  1.54025
  [1, 4]  =  1.89031
  [1, 5]  =  0.405067
  [2, 5]  =  0.405067
  [4, 5]  =  0.676646</code></pre><h2><a class="nav-anchor" id="Co-occurrence-matrix-(COOM)-1" href="#Co-occurrence-matrix-(COOM)-1">Co occurrence matrix (COOM)</a></h2><p>The elements of the Co occurrence matrix indicate how many times two words co-occur in a (sliding) word window of a given size. The COOM can be calculated for objects of type <code>Corpus</code>, <code>AbstractDocument</code> (with the exception of <code>NGramDocument</code>).</p><pre><code class="language-none">CooMatrix(crps; window, normalize)
CooMatrix(doc; window, normalize)</code></pre><p>It takes following keyword arguments:</p><ul><li><code>window::Integer</code> -length of the Window size, defaults to <code>5</code>. The actual size of the sliding window is 2 * window + 1, with the keyword argument window specifying how many words to consider to the left and right of the center one</li><li><code>normalize::Bool</code> -normalizes counts to distance between words, defaults to <code>true</code></li></ul><p>It returns the <code>CooMatrix</code> structure from which the matrix can be extracted using <code>coom(::CooMatrix)</code>. The <code>terms</code> can also be extracted from this. Here is an example usage -</p><pre><code class="language-julia">
julia&gt; crps = Corpus([StringDocument(&quot;this is a string document&quot;),

julia&gt; C = CooMatrix(crps, window=1, normalize=false)
CooMatrix{Float64}(
  [2, 1]  =  2.0
  [6, 1]  =  2.0
  [1, 2]  =  2.0
  [3, 2]  =  2.0
  [2, 3]  =  2.0
  [6, 3]  =  2.0
  [5, 4]  =  4.0
  [4, 5]  =  4.0
  [6, 5]  =  4.0
  [1, 6]  =  2.0
  [3, 6]  =  2.0
  [5, 6]  =  4.0, [&quot;string&quot;, &quot;document&quot;, &quot;token&quot;, &quot;this&quot;, &quot;is&quot;, &quot;a&quot;], OrderedDict(&quot;string&quot;=&gt;1,&quot;document&quot;=&gt;2,&quot;token&quot;=&gt;3,&quot;this&quot;=&gt;4,&quot;is&quot;=&gt;5,&quot;a&quot;=&gt;6))

julia&gt; coom(C)
6×6 SparseArrays.SparseMatrixCSC{Float64,Int64} with 12 stored entries:
  [2, 1]  =  2.0
  [6, 1]  =  2.0
  [1, 2]  =  2.0
  [3, 2]  =  2.0
  [2, 3]  =  2.0
  [6, 3]  =  2.0
  [5, 4]  =  4.0
  [4, 5]  =  4.0
  [6, 5]  =  4.0
  [1, 6]  =  2.0
  [3, 6]  =  2.0
  [5, 6]  =  4.0

julia&gt; C.terms
6-element Array{String,1}:
 &quot;string&quot;
 &quot;document&quot;
 &quot;token&quot;
 &quot;this&quot;
 &quot;is&quot;
 &quot;a&quot;
</code></pre><p>It can also be called to calculate the terms for a specific list of words / terms in the document. In other cases it calculates the the co occurrence elements for all the terms.</p><pre><code class="language-none">CooMatrix(crps, terms; window, normalize)
CooMatrix(doc, terms; window, normalize)</code></pre><pre><code class="language-julia">julia&gt; C = CooMatrix(crps, [&quot;this&quot;, &quot;is&quot;, &quot;a&quot;], window=1, normalize=false)
CooMatrix{Float64}(
  [2, 1]  =  4.0
  [1, 2]  =  4.0
  [3, 2]  =  4.0
  [2, 3]  =  4.0, [&quot;this&quot;, &quot;is&quot;, &quot;a&quot;], OrderedCollections.OrderedDict(&quot;this&quot;=&gt;1,&quot;is&quot;=&gt;2,&quot;a&quot;=&gt;3))
</code></pre><p>The type can also be specified for <code>CooMatrix</code> with the weights of type <code>T</code>. <code>T</code> defaults to <code>Float64</code>.</p><pre><code class="language-none">CooMatrix{T}(crps; window, normalize) where T &lt;: AbstractFloat
CooMatrix{T}(doc; window, normalize) where T &lt;: AbstractFloat
CooMatrix{T}(crps, terms; window, normalize) where T &lt;: AbstractFloat
CooMatrix{T}(doc, terms; window, normalize) where T &lt;: AbstractFloat</code></pre><p>Remarks:</p><ul><li>The sliding window used to count co-occurrences does not take into consideration sentence stops however, it does with documents i.e. does not span across documents</li><li>The co-occurrence matrices of the documents in a corpus are summed up when calculating the matrix for an entire corpus</li></ul><div class="admonition note"><div class="admonition-title">Note</div><div class="admonition-text"><p>The Co occurrence matrix does not work for <code>NGramDocument</code>, or a Corpus containing an <code>NGramDocument</code>.</p></div></div><pre><code class="language-julia">julia&gt; C = CooMatrix(NGramDocument(&quot;A document&quot;), window=1, normalize=false) # fails, documents are NGramDocument
ERROR: The tokens of an NGramDocument cannot be reconstructed</code></pre><h2><a class="nav-anchor" id="Sentiment-Analyzer-1" href="#Sentiment-Analyzer-1">Sentiment Analyzer</a></h2><p>It can be used to find the sentiment score (between 0 and 1) of a word, sentence or a Document. A trained model (using Flux) on IMDB word corpus with weights saved are used to calculate the sentiments.</p><pre><code class="language-none">model = SentimentAnalyzer(doc)
model = SentimentAnalyzer(doc, handle_unknown)</code></pre><ul><li>doc              = Input Document for calculating document (AbstractDocument type)</li><li>handle_unknown   = A function for handling unknown words. Should return an array (default (x)-&gt;[])</li></ul><h2><a class="nav-anchor" id="Summarizer-1" href="#Summarizer-1">Summarizer</a></h2><p>TextAnalysis offers a simple text-rank based summarizer for its various document types.</p><pre><code class="language-none">summarize(d, ns)</code></pre><p>It takes 2 arguments:</p><ul><li><code>d</code> : A document of type <code>StringDocument</code>, <code>FileDocument</code> or <code>TokenDocument</code></li><li><code>ns</code> : (Optional) Mention the number of sentences in the Summary, defaults to <code>5</code> sentences.</li></ul><pre><code class="language-julia">julia&gt; s = StringDocument(&quot;Assume this Short Document as an example. Assume this as an example summarizer. This has too foo sentences.&quot;)

julia&gt; summarize(s, ns=2)
2-element Array{SubString{String},1}:
 &quot;Assume this Short Document as an example.&quot;
 &quot;This has too foo sentences.&quot;</code></pre><h2><a class="nav-anchor" id="Tagging_schemes-1" href="#Tagging_schemes-1">Tagging_schemes</a></h2><p>There are many tagging schemes used for sequence labelling. TextAnalysis currently offers functions for conversion between these tagging format.</p><ul><li>BIO1</li><li>BIO2</li><li>BIOES</li></ul><pre><code class="language-julia">julia&gt; tags = [&quot;I-LOC&quot;, &quot;O&quot;, &quot;I-PER&quot;, &quot;B-MISC&quot;, &quot;I-MISC&quot;, &quot;B-PER&quot;, &quot;I-PER&quot;, &quot;I-PER&quot;]

julia&gt; tag_scheme!(tags, &quot;BIO1&quot;, &quot;BIOES&quot;)

julia&gt; tags
8-element Array{String,1}:
 &quot;S-LOC&quot;
 &quot;O&quot;
 &quot;S-PER&quot;
 &quot;B-MISC&quot;
 &quot;E-MISC&quot;
 &quot;B-PER&quot;
 &quot;I-PER&quot;
 &quot;E-PER&quot;</code></pre><h2><a class="nav-anchor" id="Parts-of-Speech-Tagger-1" href="#Parts-of-Speech-Tagger-1">Parts of Speech Tagger</a></h2><p>This tagger can be used to find the POS tag of a word or token in a given sentence. It is a based on <code>Average Perceptron Algorithm</code>. The model can be trained from scratch and weights are saved in specified location. The pretrained model can also be loaded and can be used directly to predict tags.</p><h3><a class="nav-anchor" id="To-train-model:-1" href="#To-train-model:-1">To train model:</a></h3><pre><code class="language-julia">julia&gt; tagger = PerceptronTagger(false) #we can use tagger = PerceptronTagger()
julia&gt; fit!(tagger, [[(&quot;today&quot;,&quot;NN&quot;),(&quot;is&quot;,&quot;VBZ&quot;),(&quot;good&quot;,&quot;JJ&quot;),(&quot;day&quot;,&quot;NN&quot;)]])
iteration : 1
iteration : 2
iteration : 3
iteration : 4
iteration : 5</code></pre><h3><a class="nav-anchor" id="To-load-pretrained-model:-1" href="#To-load-pretrained-model:-1">To load pretrained model:</a></h3><pre><code class="language-julia">julia&gt; tagger = PerceptronTagger(true)
loaded successfully
PerceptronTagger(AveragePerceptron(Set(Any[&quot;JJS&quot;, &quot;NNP_VBZ&quot;, &quot;NN_NNS&quot;, &quot;CC&quot;, &quot;NNP_NNS&quot;, &quot;EX&quot;, &quot;NNP_TO&quot;, &quot;VBD_DT&quot;, &quot;LS&quot;, (&quot;Council&quot;, &quot;NNP&quot;)  …  &quot;NNPS&quot;, &quot;NNP_LS&quot;, &quot;VB&quot;, &quot;NNS_NN&quot;, &quot;NNP_SYM&quot;, &quot;VBZ&quot;, &quot;VBZ_JJ&quot;, &quot;UH&quot;, &quot;SYM&quot;, &quot;NNP_NN&quot;, &quot;CD&quot;]), Dict{Any,Any}(&quot;i+2 word wetlands&quot;=&gt;Dict{Any,Any}(&quot;NNS&quot;=&gt;0.0,&quot;JJ&quot;=&gt;0.0,&quot;NN&quot;=&gt;0.0),&quot;i-1 tag+i word NNP basic&quot;=&gt;Dict{Any,Any}(&quot;JJ&quot;=&gt;0.0,&quot;IN&quot;=&gt;0.0),&quot;i-1 tag+i word DT chloride&quot;=&gt;Dict{Any,Any}(&quot;JJ&quot;=&gt;0.0,&quot;NN&quot;=&gt;0.0),&quot;i-1 tag+i word NN choo&quot;=&gt;Dict{Any,Any}(&quot;NNP&quot;=&gt;0.0,&quot;NN&quot;=&gt;0.0),&quot;i+1 word antarctica&quot;=&gt;Dict{Any,Any}(&quot;FW&quot;=&gt;0.0,&quot;NN&quot;=&gt;0.0),&quot;i-1 tag+i word -START- appendix&quot;=&gt;Dict{Any,Any}(&quot;NNP&quot;=&gt;0.0,&quot;NNPS&quot;=&gt;0.0,&quot;NN&quot;=&gt;0.0),&quot;i-1 word wahoo&quot;=&gt;Dict{Any,Any}(&quot;JJ&quot;=&gt;0.0,&quot;VBD&quot;=&gt;0.0),&quot;i-1 tag+i word DT children&#39;s&quot;=&gt;Dict{Any,Any}(&quot;NNS&quot;=&gt;0.0,&quot;NN&quot;=&gt;0.0),&quot;i word dnipropetrovsk&quot;=&gt;Dict{Any,Any}(&quot;NNP&quot;=&gt;0.003,&quot;NN&quot;=&gt;-0.003),&quot;i suffix hla&quot;=&gt;Dict{Any,Any}(&quot;JJ&quot;=&gt;0.0,&quot;NN&quot;=&gt;0.0)…), DefaultDict{Any,Any,Int64}(), DefaultDict{Any,Any,Int64}(), 1, [&quot;-START-&quot;, &quot;-START2-&quot;]), Dict{Any,Any}(&quot;is&quot;=&gt;&quot;VBZ&quot;,&quot;at&quot;=&gt;&quot;IN&quot;,&quot;a&quot;=&gt;&quot;DT&quot;,&quot;and&quot;=&gt;&quot;CC&quot;,&quot;for&quot;=&gt;&quot;IN&quot;,&quot;by&quot;=&gt;&quot;IN&quot;,&quot;Retrieved&quot;=&gt;&quot;VBN&quot;,&quot;was&quot;=&gt;&quot;VBD&quot;,&quot;He&quot;=&gt;&quot;PRP&quot;,&quot;in&quot;=&gt;&quot;IN&quot;…), Set(Any[&quot;JJS&quot;, &quot;NNP_VBZ&quot;, &quot;NN_NNS&quot;, &quot;CC&quot;, &quot;NNP_NNS&quot;, &quot;EX&quot;, &quot;NNP_TO&quot;, &quot;VBD_DT&quot;, &quot;LS&quot;, (&quot;Council&quot;, &quot;NNP&quot;)  …  &quot;NNPS&quot;, &quot;NNP_LS&quot;, &quot;VB&quot;, &quot;NNS_NN&quot;, &quot;NNP_SYM&quot;, &quot;VBZ&quot;, &quot;VBZ_JJ&quot;, &quot;UH&quot;, &quot;SYM&quot;, &quot;NNP_NN&quot;, &quot;CD&quot;]), [&quot;-START-&quot;, &quot;-START2-&quot;], [&quot;-END-&quot;, &quot;-END2-&quot;], Any[])</code></pre><h3><a class="nav-anchor" id="To-predict-tags:-1" href="#To-predict-tags:-1">To predict tags:</a></h3><p>The perceptron tagger can predict tags over various document types-</p><pre><code class="language-none">predict(tagger, sentence::String)
predict(tagger, Tokens::Array{String, 1})
predict(tagger, sd::StringDocument)
predict(tagger, fd::FileDocument)
predict(tagger, td::TokenDocument)</code></pre><p>This can also be done by -     tagger(input)</p><pre><code class="language-julia">julia&gt; predict(tagger, [&quot;today&quot;, &quot;is&quot;])
2-element Array{Any,1}:
 (&quot;today&quot;, &quot;NN&quot;)
 (&quot;is&quot;, &quot;VBZ&quot;)

julia&gt; tagger([&quot;today&quot;, &quot;is&quot;])
2-element Array{Any,1}:
 (&quot;today&quot;, &quot;NN&quot;)
 (&quot;is&quot;, &quot;VBZ&quot;)</code></pre><p><code>PerceptronTagger(load::Bool)</code></p><ul><li>load      = Boolean argument if <code>true</code> then pretrained model is loaded</li></ul><p><code>fit!(self::PerceptronTagger, sentences::Vector{Vector{Tuple{String, String}}}, save_loc::String, nr_iter::Integer)</code></p><ul><li>self      = <code>PerceptronTagger</code> object</li><li>sentences = <code>Vector</code> of <code>Vector</code> of <code>Tuple</code> of pair of word or token and its POS tag [see above example]</li><li>save_loc  = location of file to save the trained weights</li><li>nr_iter   = Number of iterations to pass the <code>sentences</code> to train the model ( default 5)</li></ul><p><code>predict(self::PerceptronTagger, tokens)</code></p><ul><li>self      = PerceptronTagger</li><li>tokens    = <code>Vector</code> of words or tokens for which to predict tags</li></ul><footer><hr/><a class="previous" href="../corpus/"><span class="direction">Previous</span><span class="title">Corpus</span></a><a class="next" href="../semantic/"><span class="direction">Next</span><span class="title">Semantic Analysis</span></a></footer></article></body></html>
